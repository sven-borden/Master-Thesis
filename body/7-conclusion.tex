\chapter{Conclusion}

\section{Simulation}
    The simulation developed and implemented during this thesis is giving promising results as shown in the results section. The complete workflow is implemented in Python 3 as it gives the more tools and flexibility for later use. The simulation has been validated with experimental results as much as possible. As it was limited to only a few, only the most basics (but the most important) parts were tested. This includes the forward and the backward displacement of the robot when using only sequences A or B. The friction model or displacement model seems to correlate correction as well as the methods to determine which legs are touching the ground. Finally, before the validation of the other sequences on a robust robot, the sequences were validated using the theory.\\
    
    The behavior mapping of all possible combinations of sequences and phase actuation shows that there is a large number of sequences that produce different outputs in terms of displacement and orientation change for the robot. It seems that the robot would not only be able to move forward and backward but also turns on its left or right. Those different possibilities need to be validated with experimental results, starting with simple ones, such as the sequence \textit{ABAB} where the right legs are in sequence A while the left legs are in sequence B. From the simulation, this specific sequence should produce a change in the robot's orientation.\\
    
    The way the displacement and orientation of the robot are computed, using a simple coefficient seems to works well for cases that are accessible to process by hand. It also works well with specifics cases where, for example, all legs are moving in opposites directions and the robot should not have any motion. The coefficients and the modularity of the simulation would make it easier to tune the simulation results to real world experiments. Finally, the simulation is almost ready to run at half cycle, which means we could imagine the actuator doing only half of the cycle before switching the sequence of its joint and do the second half of the cycle on this new specific sequence. This would open even more possibilities and create a larger mapping table. It would also add more complexity to the controller, which may require some neural network tuning.
    
\section{Controller}
    The controller based on Deep Q Learning is performing successfully as it is able to reach out to the goals fixed by the program. The first scenarios are very similar and are a good start for the controller to learn to drive the agent. Once this stage is completed, new scenarios are in charge of fine-tuning the neural network with a random goal position. The Deep Q Learning controller completes both parts successfully, especially when we restrict the number of sequences for example to use only Sequence A and B. It was a surprise to see the learning progression completed after 40 iterations. \\
    
    As the implemented controller is a closed-loop controller, it would be easier to translate it to the experimental robot. Even if the robot is not consistent, robust or subject to external perturbation, the controller should still be able to train itself to drive the robot to its target. A good practical process would recommend to first train a model with the simulation results to get an already working controller before testing on the real robot. The Deep Q Learning controller should learn with an already good understanding of the motion of the robot. This is especially true if the robot is not consistent.
    
\section{Future work}
    A complete workflow process is in place, from the simulation to the generation of the motions for the robot. Then a controller that takes as input the robot's motions is able to learn and create a method to drive the robot to specific goals. First thing to do is to validate as much as possible the simulation model. It is already known that forward and backward motion can be achieved. There is now the need to find a sequence that creates consistently a orientation change for the robot. Once this is done, a new table can be generated to feed the Deep Q Learning controller.\\
    
    A specific setup needs to be created to use the controller in a real world experiment, as it is working in a closed-loop, a camera and some distance sensors must be implemented to the robot to feed correctly the neural network.