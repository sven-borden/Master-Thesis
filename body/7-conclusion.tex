\chapter{Conclusion}

\section{Simulation}
    The simulation developed and implemented during this thesis is giving promising results as shown in the results section. It has been as much as possible validated with experimental results. Forward and backward motion using sequences A and B are good example of correlated results coming from the simulation transposed to the real robot. The mapping of all possible combination of sequences and phase actuation shows that there is a large amount of sequences that produce different output in term of displacement and orientation change for the robot. Those different possibilities need to be validated with experimental results, starting with simple ones, such as the sequence \textit{ABAB} where the right legs are in sequence A while the left legs are in sequences B should produce a change in orientation.
    
    The way the displacement and orientation of the robot are computed, using simple coefficient seems to works well for cases that are humanly accessible to process. It also works well with extreme cases where, for example, all legs are moving in opposites direction and the robot should not have any motion. The uses of coefficient and the modularity of the simulation would make it more easy to tune the simulation results to real world experiments. The simulation is almost ready to accept half cycle, which means we could imagine the actuator only doing half of the cycle before switching the sequence of its joint and do the second half of the cycle on another sequence. This would open even more motion possibilities. It would also add more complexity to the controller.
    
\section{Controller}
    The controller based on Deep Q Learning is performing successfully as it is able to reach out to the goals fixed by the program. Also once the same scenario successfully completed, the controller is able to pass multiple random scenario where the position of the goal is set randomly. It is also quite interesting and impressive to see that with a subset of sequences, the controller learns the to drive the robot in a few minutes with around 40 iterations. As it is a closed-loop controller, it could be used in an experimental exercise even if the robot motion is not completely robust or if the robot is subject to external perturbations. A good practical process would be to compute a first model with the simulation results. As seen this gives good theoretical results very fast. Then the real world training can be done starting from the pre-trained model to converge faster to a acceptable controller.
\section{Future work}
    A complete workflow process is in place, from the simulation to the generation of the motions for the robot. Then a controller that takes as input the robot's motions is able to learn and create a method to drive the robot to specific goals. As the real world robot wasn't robust enough and the number of experiments were limited to forward and backward motions, the first future work to do is doing experiments that gives rotation of the robot as shown in the simulation.